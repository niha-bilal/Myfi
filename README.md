# ğŸ§  Intent Detection with Transformer Models

Identify user intents from short text inputs using traditional and transformer-based models (BERT, Pythia). This project combines data augmentation, model fine-tuning, and an interactive Gradio interface.

---

## ğŸ“ Repository Structure

```bash
.
â”œâ”€â”€ .gradio/                         # Hugging Face Spaces settings
â”œâ”€â”€ intent_model/                   # Trained model directory (BERT or similar)
â”œâ”€â”€ sof-intent-detection/intent_model/ # Additional model directory
â”œâ”€â”€ .gitattributes                  # Git attributes
â”œâ”€â”€ README.md                       # Project overview and documentation
â”œâ”€â”€ app.py                          # Gradio app main file
â”œâ”€â”€ app11.py                        # Renamed/variant app script
â”œâ”€â”€ app_backup.py                   # Backup of older app version
â”œâ”€â”€ config.json                     # Model/configuration file
â”œâ”€â”€ label_classes.npy               # Encoded label classes
â”œâ”€â”€ model.safetensors               # Model weights in safetensors format
â”œâ”€â”€ prompt_training_examples.json   # Augmented prompt examples
â”œâ”€â”€ requirements.txt                # Project dependencies
â”œâ”€â”€ special_tokens_map.json         # Tokenizer metadata
â”œâ”€â”€ temp_backup.py                  # Temporary backup script
â”œâ”€â”€ tokenizer_config.json           # Tokenizer configuration
â”œâ”€â”€ train_model.py                  # Training script
â”œâ”€â”€ vocab.txt                       # Vocabulary file


```
---

## ğŸš€ Features

- Multi-class classification across 21 intent labels
- Fine-tuned BERT and Pythia-70M models
- Augmented dataset: 328 â†’ 1004 examples
- Real-time Gradio interface
- Custom PyTorch training for flexibility

---

## ğŸ“¦ Installation & Setup

1. Clone this repository:

git clone https://github.com/niha-bilal/Tifin.git


cd Tifin


2. Install dependencies:

3. pip install -r requirements.txt


4. Run the app:


python app.py
---
ğŸ§ª Model Accuracy
| Model        | Accuracy | Highlights                        |
| ------------ | -------- | --------------------------------- |
| Pythia-70M   | 92â€“93%   | Strong semantic performance       |
| BERT         | 90â€“92%   | Flexible, custom PyTorch training |
| SVM (TF-IDF) | 71â€“92%   | Good with augmentation            |
| Naive Bayes  | \~50%    | Lightweight but too shallow       |
---
ğŸ§° How to Train
- Train BERT:python bert.py
- Train Pythia or other models:
- python train.py
Models are saved to:

./Tifin/trained_model/ (Pythia)

./intent_model/ (BERT)

-----
ğŸŒ Gradio Web Interface


To test predictions:


python app.py

Youâ€™ll get:

Running on local URL: http://127.0.0.1:7860

Running on public URL: https://xxxx.gradio.live

---
ğŸ“ˆ Recommendations

- Use weighted loss or SMOTE to balance class distribution

- Add feedback/error correction in Gradio interface

- Test with optimized models like DistilBERT

- Apply active learning and Ray Tune for fine-tuning
